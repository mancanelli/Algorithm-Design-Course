{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LN2gw2Xo31Nw",
        "WYrCTUb01DXA",
        "9XlsoO283JxG",
        "YgKzzGyVBKbr",
        "kLyTtNqgwNG4",
        "EPArP46HGMvl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "2JDx1lI9grx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for homework 2 of the Big Data Computing course. This notebook is organized into 10 numbered sections, each of which addressing one step to use LSH for cosine similarity and perform a comparison between LSH and simple similarity computation. You can run all the sections or part of them, following the instructions described in this introduction. \n",
        "\n",
        "All the sections related to heavier computations (4, 6, 7, 9, 10) contain the line of codes used to write the result of the corresponding conputation on a file. In this way, retrieving data from the files, it is possible to skip some slow portions of the code to run only the needed ones. This notebook comes with all the files already created.\n",
        "\n",
        "**Disclaimer 1:** In this notebook, it is used the functionality of mounting of Google Drive on Colab to manage the files. If you make use of another environment pay attention to include the files if you don't want to recreate them from scratch. In any case, change the path of the file (defined in section 3) to match your needs. If the path is not correctly stated, no computation can be completed.\n",
        "\n",
        "**Disclaimer 2:**  Running the writing lines of this notebook may destroy or corrupt the integrity of the mentioned files. To avoid unintentional execution, all the writing lines are commented out. Uncomment them to perform the actual writing (you can change the name of the destination files to keep the original ones safe)."
      ],
      "metadata": {
        "id": "GaFAqOAXuhO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Details"
      ],
      "metadata": {
        "id": "zaSrxgVRD1mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Import:** It contains all needed libraries and functions for correctly executing the other sections.\n",
        "2. **Download dataset:** It contains the code to download and unzip the kaggle dataset Amazon Fine Food Reviews (https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews).\n",
        "3. **Useful functions and parameters:** It contains functions and variables shared by many following sections, to avoid repeating them several times. Here is also the definition of the file paths.\n",
        "4. **Preprocessing:** It contains the needed steps to perform the preprocessing of the data obtained \n",
        "> 1. **Retrieve data:** It reads data from the 'Reviews.csv' file downloaded from the original dataset.\n",
        "> 2. **Perform the transformation:** It contains the function 'preprocessing' used to compute the actual transformation of the texts. When the function is called, it is possible to enable/disable some preprocessing steps among the implemented ones (expand contractions, discard tags, delete punctuations, delete numbers, apply -principled- lemmatization and delete stopwords) using the 'options' dictionary as parameter of the function.\n",
        "> 3. **Write preprocessed data:** It allows to write the result of the preprocessing steps.\n",
        "5. **Create tf-idf vectors:** It reads the preprocessed data and creates the vectorized representation. In this section the data are splitted in order to have different sets for the following computations. In particular, data are partitioned into the 'test' set (0.5% of the total) and the 'train' set. Moreover, it is created the set 'thresh_test' (1% of the total) to perform a preliminary analisys to set the threshold theta above which two documents are considered similar. From these sets, three different sparse vectors are obtained.\n",
        "6. **Set the threshold:** It helps to find the right value for theta\n",
        "> 1. **Compute similarities:** It computes the similarities between documents of 'thresh_test' (pairwise). Then it writes the result on a file.\n",
        "> 2. **Read results:** It reads the file containing the previous results.\n",
        "> 3. **Choose theta:** It analyzes the results to set the value of theta. There can be different way to address this problem. The assumption made here is that at least one document in 'thresh_test' has its most-similar document in 'thresh_test'. In order to not cut out too many results, we select the smallest value among the highest similar values of each document. In this way every document d (in 'thresh_test' and hopefully in all the dataset) has at least one similar document d'.\n",
        "7. **Ground truth:** for each document in 'test', it computes the exact set of similar documents belonging to 'train' set using cosine similarity and the chosen threshold. During the process, it writes the results on a file.\n",
        "8. **Signature matrix:** It defines the class to implement the computation of the signature matrix of one given document d. It has the variable 'projections' (the set of vectors used to perform the dot product with d), and the function 'generate_signature' that takes d and returns its signature as a sequence of bits. Note that both Signature class and the following LSH class can be instantiated from scratch or reusing previously computed variables retrieved from a file (as described in the folowing lines dedicated to section 9.3).\n",
        "9. **LSH:** It contains all the needed steps to implement Locality Sensitive Hashing. It does not perform the actual similarity computation (the following section is responsible for this)\n",
        "> 1. **Create LSH class:** It defines the class of LSH. The main components are the variable 'lst_dict', that stores a list of dictionaries related to each band in which we split the signature matrix of the 'train' documents, and the function 'compute_sim', that takes in input a document d of the 'test' set and computes the approximative set of similar documents using the banding technique.\n",
        "> 2. **Create new instances:** It creates the new instances of Signature and LSH classes. Then it writes the variables 'projections' and 'lst_dict' in one file.\n",
        "> 3. **Retrieve old instances:** It retrieves 'projections' and 'lst_dict' from the file in which they are stored and use them to create the instances respectively of Signature and LSH classes with the same parameters of the previous computation. \n",
        "10. **Comparison:** It returns the final results\n",
        "> 1. **LSH result:** It performs LSH computation and compares the set of returned similar documents with the true set of similar documents computed in section 7. It uses the Jaccard coefficient and it takes into consideration how fast LSH is against the traditional approach.\n",
        "> 2. **Aggregated results:** It shows the average values of the principal indicators over the number of test documents\n",
        "\n",
        "Some specific implementations details are described in the comments in order to understand them while reading the code."
      ],
      "metadata": {
        "id": "yEgRZ4y3EG-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to run"
      ],
      "metadata": {
        "id": "HccR-6-HDVYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each task is indicated the number of the sections strictly necessary to perform that task\n",
        "\n",
        "* **Preprocess data:** 1 -> 3 -> 4\n",
        "* **Create tf-idf vectors:** 1 -> 3 -> 5\n",
        "* **Set threshold (computing pairwise similarities of 'thresh_test' set from scratch):** 1 -> 3 -> 5 -> 6\n",
        "* **Set threshold (reusing stored data):** 1 -> 3 -> 5 -> 6.2 -> 6.3\n",
        "* **Computing exact similarities (from scratch):** 1 -> 3 -> 5 -> 7\n",
        "* **Create instances of Signature class and LSH class (from scratch):** 1 -> 3 -> 5 -> 8 -> 9.1 -> 9.2\n",
        "* **Create instances of Signature class and LSH class (reusing stored data):** 1 -> 3 -> 5 -> 8 -> 9.1 -> 9.3\n",
        "* **Use LSH and compare results with ground truth:** 1 -> 3 -> 5 -> 8 -> 9.1 -> 9.3 -> 10.1\n",
        "* **Get aggregated results (reusing stored data):** 1 -> 3 -> 5 -> 10.2\n",
        "\n",
        "\n",
        "Final results:\n",
        "\n",
        "* Average time for exact similarity computation: 73.19\n",
        "* Average time for LSH similarity computation: 27.70\n",
        "* Average Jaccard coefficient: 0.73"
      ],
      "metadata": {
        "id": "qvLHcte2Rfq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Import"
      ],
      "metadata": {
        "id": "LN2gw2Xo31Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comment this lines if you don't use Google Colab\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import json\n",
        "import collections\n",
        "from zipfile import ZipFile\n",
        "import linecache\n",
        "\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TEv8I4P34nQ",
        "outputId": "f716606b-8140-4cc9-9f9c-416074a1e1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Download dataset"
      ],
      "metadata": {
        "id": "WYrCTUb01DXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "KBknqUzd07Rc",
        "outputId": "d323ed90-3520-4ebb-a8e1-049314ec43cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-151d7575-b357-455b-9955-e2811cc63f3d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-151d7575-b357-455b-9955-e2811cc63f3d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# upload kaggle.json\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and unzip the dataset\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews\n",
        "\n",
        "with ZipFile('amazon-fine-food-reviews.zip', 'r') as zipObj:\n",
        "  zipObj.extractall()"
      ],
      "metadata": {
        "id": "UOgHwPeM3SgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dabf3a9-7774-4ed0-f980-7162455f80f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading amazon-fine-food-reviews.zip to /content\n",
            " 95% 231M/242M [00:01<00:00, 180MB/s]\n",
            "100% 242M/242M [00:01<00:00, 173MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Useful functions and parameters"
      ],
      "metadata": {
        "id": "N3TwBeDcZNfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some elements defined in this section may be overridden by running the code in the following sections (e.g., theta)"
      ],
      "metadata": {
        "id": "y0A2wswJy7uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the similarity between two vectors\n",
        "def cos_sim(u, v):\n",
        "  norm = np.linalg.norm(u) * np.linalg.norm(v)\n",
        "  cosine = np.dot(u, v.T) / norm\n",
        "  ang = np.arccos(cosine)\n",
        "  return 1 - ang/np.pi"
      ],
      "metadata": {
        "id": "j60YjrFMXAuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some useful parameters for the following computations\n",
        "# if theta changes in section 6, adjust the other parameters\n",
        "theta = 0.60\n",
        "b = 24\n",
        "r = 6\n",
        "m = b * r\n",
        "print(round((1/b)**(1/r), 2), m)"
      ],
      "metadata": {
        "id": "c0zSgeJOZ3qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e12358-fe6b-4a1a-ce9e-1d56a9167a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.59 144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define file paths\n",
        "dataset_file = '/content/drive/MyDrive/Reviews.csv'\n",
        "preprocessed_file = '/content/drive/MyDrive/preprocessed.csv'\n",
        "thresh_sim_file = '/content/drive/MyDrive/thresh.csv'\n",
        "true_sim_file = '/content/drive/MyDrive/true_sim.txt'\n",
        "sig_file = '/content/drive/MyDrive/sig.txt'\n",
        "lsh_file = '/content/drive/MyDrive/lsh.txt'\n",
        "comparison_file = '/content/drive/MyDrive/comparison.txt'"
      ],
      "metadata": {
        "id": "dokTcoqGWmcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Preprocessing"
      ],
      "metadata": {
        "id": "9XlsoO283JxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1) Retrieve data"
      ],
      "metadata": {
        "id": "iKhBXT9SZddd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read and clear the dataset\n",
        "df = pd.read_csv(dataset_file, sep=',')\n",
        "df.dropna(subset=['Score', 'Summary', 'Text'], inplace=True)\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "oIEOXcyC6loQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract useful informations\n",
        "scores = df['Score'].tolist()\n",
        "summaries = df['Summary'].tolist()\n",
        "texts = df['Text'].tolist()"
      ],
      "metadata": {
        "id": "utK4fDzuDhbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2) Perform the transformation"
      ],
      "metadata": {
        "id": "7k84u8U8Zvpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contracts = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'alls\": \"you alls\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
        "\n",
        "# define preprocessing pipeline\n",
        "def preprocessing(string, contracts, puncts, options):\n",
        "  # convert all characters into lowercase characters \n",
        "  string = string.lower()\n",
        "  \n",
        "  # expand any contractions using 'contracts' dictionary\n",
        "  if options.get(\"expand_contractions\"):\n",
        "    for pair in contracts.items():\n",
        "      if pair[0] in string:\n",
        "        string = string.replace(pair[0], pair[1])\n",
        "    \n",
        "    ''' alternative\n",
        "    regex = re.compile('(%s)' % '|'.join(contracts.keys()))\n",
        "    string = regex.sub(lambda x: contracts[x.group(0)], string)\n",
        "    '''\n",
        "  \n",
        "  # delete any tags (they carry no infos)\n",
        "  if options.get(\"discard_tags\"):\n",
        "    for tag in [\"<p>\", \"<br />\", \"</a>\"]:\n",
        "      if tag in string:\n",
        "        string = string.replace(tag, \" \")\n",
        "    \n",
        "    string = \" \".join(list(filter(lambda elem: \"a href\" not in elem, re.split(\"<|>\", string))))\n",
        "\n",
        "  # delete any punctuation symbols using 'puncts' list\n",
        "  if options.get(\"del_punctuations\"):\n",
        "    for punct in puncts:\n",
        "      if punct in string:\n",
        "        string = string.replace(punct, \" \")\n",
        "\n",
        "    ''' alternative\n",
        "    regex = re.compile('(%s)' % '|'.join(map(re.escape, puncts)))\n",
        "    string = regex.sub(lambda x: \"\", string)\n",
        "    '''\n",
        "  \n",
        "  # delete any numbers\n",
        "  if options.get(\"del_numbers\"):\n",
        "    for num in list(range(0, 10)):\n",
        "      if str(num) in string:\n",
        "        string = string.replace(str(num), \" \")\n",
        "    \n",
        "    ''' alternative\n",
        "    string = re.sub(r'\\d+', \"\", string)\n",
        "    '''\n",
        "\n",
        "  # apply lemmatization\n",
        "  if options.get(\"use_lemmatization\"):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # principled strategy based on part-of-speech tags (inspired by https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
        "    pos_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    string = \" \".join([lemmatizer.lemmatize(pair[0], pos_dict.get(pair[1][0].upper(), wordnet.NOUN)) for pair in pos_tag(word_tokenize(string))])\n",
        "\n",
        "    ''' alternative\n",
        "    string = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(string)])\n",
        "    '''\n",
        "\n",
        "  # delete stopwords\n",
        "  if options.get(\"del_stopwords\"):\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    string = \" \".join([word for word in word_tokenize(string) if word not in stopwords])\n",
        "\n",
        "  return string"
      ],
      "metadata": {
        "id": "5Tjf-EJOLKUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose how to preprocess data\n",
        "options = {\"expand_contractions\": True,\n",
        "           \"discard_tags\": True,\n",
        "           \"del_punctuations\": True,\n",
        "           \"del_numbers\": True,\n",
        "           \"use_lemmatization\": True,\n",
        "           \"del_stopwords\": True}\n",
        "\n",
        "# apply the actual preprocessing step\n",
        "summaries = [preprocessing(elem, contracts, puncts, options) for elem in summaries]\n",
        "texts = [preprocessing(elem, contracts, puncts, options) for elem in texts]"
      ],
      "metadata": {
        "id": "-Zoa3kWJ5cXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3) Write preprocessed data"
      ],
      "metadata": {
        "id": "mRJhU2vQaLnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write preprocessed data into a csv file\n",
        "# uncomment the following lines to perform the actual writing\n",
        "'''\n",
        "f_csv = open(preprocessed_file, \"w\")\n",
        "f_csv.write(\"score,summary,text\\n\")\n",
        "\n",
        "for i in range(len(scores)):\n",
        "  f_csv.write(str(scores[i]) + \",\" + summaries[i] + \",\" + texts[i] + \"\\n\")\n",
        "\n",
        "f_csv.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "dSYwE9Hy-l7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Create tf-idf vectors"
      ],
      "metadata": {
        "id": "YgKzzGyVBKbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read preprocessed data\n",
        "df = pd.read_csv(preprocessed_file, sep=',')\n",
        "\n",
        "scores = df['score'].tolist()\n",
        "summaries = df['summary'].tolist()\n",
        "texts = df['text'].tolist()\n",
        "\n",
        "# design choice: each document contains a reconciled string composed of 'scores', 'summaries' and 'texts'\n",
        "# comment the following line to ignore 'scores' and 'summaries' (the number of terms in the vectorized representations will be less)\n",
        "texts = [str(elem[0]) + \" \" + str(elem[1]) + \" \" + str(elem[2]) for elem in zip(scores, summaries, texts)]"
      ],
      "metadata": {
        "id": "nfjDPeUaENYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents in train set and test set\n",
        "# use thresh_test to carry out a preliminary analysis on the threshold (theta)\n",
        "thresh_test_prop = 0.01\n",
        "test_prop = 0.005\n",
        "\n",
        "_, thresh_test = train_test_split(texts, test_size=thresh_test_prop, random_state=32)\n",
        "train, test = train_test_split(texts, test_size=test_prop, random_state=64)"
      ],
      "metadata": {
        "id": "FWTFUTVyKEk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the tf-idf vectors (sparse representation)\n",
        "vectorizer = TfidfVectorizer(min_df=100)\n",
        "texts_tfidf = vectorizer.fit_transform(texts)\n",
        "train_tfidf = vectorizer.transform(train)\n",
        "test_tfidf = vectorizer.transform(test)\n",
        "thresh_test_tfidf = vectorizer.transform(thresh_test)"
      ],
      "metadata": {
        "id": "5Blbqs_f6bFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Set the threshold"
      ],
      "metadata": {
        "id": "kLyTtNqgwNG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1) Compute similarities"
      ],
      "metadata": {
        "id": "UYtFDFo7cG29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_list = []\n",
        "\n",
        "# compute the similarity between each pair of documents in the 'thresh_test' set and \n",
        "for i in range(thresh_test_tfidf.shape[0]):\n",
        "  u = thresh_test_tfidf[i:i+1].toarray()\n",
        "\n",
        "  for j in range(thresh_test_tfidf.shape[0]):\n",
        "    if j > i: # compute only one half of the similarities to speed up the process (the remaining ones are dual)\n",
        "      v = thresh_test_tfidf[j:j+1].toarray()\n",
        "      val = round(cos_sim(u,v).item(), 2)\n",
        "      \n",
        "      if 0.6 <= val < 1.0:\n",
        "        sim_list.append([i, j, val])\n",
        "\n",
        "# compute the second half and unify\n",
        "sh = [[sim[1], sim[0], sim[2]] for sim in sim_list]\n",
        "sim_list = sim_list + sh"
      ],
      "metadata": {
        "id": "hiQjCLRF6CaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the similarities obtained in the previous lines on a csv file\n",
        "# uncomment the following lines to perform the actual writing\n",
        "'''\n",
        "f_csv = open(thresh_sim_file, \"w\")\n",
        "f_csv.write(\"i,j,val\\n\")\n",
        "\n",
        "for i in range(len(sim_list)):\n",
        "  f_csv.write(str(sim_list[i][0]) + \",\" + str(sim_list[i][1]) + \",\" + str(sim_list[i][2]) + \"\\n\")\n",
        "  f_csv.write(str(sim_list[i][1]) + \",\" + str(sim_list[i][0]) + \",\" + str(sim_list[i][2]) + \"\\n\")\n",
        "\n",
        "f_csv.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "v9XOEoCgPbMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2) Read results"
      ],
      "metadata": {
        "id": "ohmV9nB3cNhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the similarities from the file\n",
        "df = pd.read_csv(thresh_sim_file, sep=',')\n",
        "\n",
        "i = df['i'].tolist()\n",
        "j = df['j'].tolist()\n",
        "val = df['val'].tolist()\n",
        "\n",
        "sim_list = []\n",
        "for k in range(len(i)):\n",
        "  sim_list.append([i[k], j[k], val[k]])"
      ],
      "metadata": {
        "id": "TMemkCGbUNx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3) Choose theta"
      ],
      "metadata": {
        "id": "N4xNF0tmcwBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary for similarity values\n",
        "sim_dict = {}\n",
        "\n",
        "for sim in sim_list:\n",
        "  sim_dict.setdefault(sim[0], [])\n",
        "  sim_dict[sim[0]].append(sim[2])\n",
        "\n",
        "# create a dictionary for statistical insigths\n",
        "stat_dict = {}\n",
        "\n",
        "for key, value in sim_dict.items():\n",
        "  stat_dict.setdefault(key, [])\n",
        "  stat_dict[key].append(np.mean(value))\n",
        "  stat_dict[key].append(np.median(value))\n",
        "  stat_dict[key].append(np.percentile(value, 90))"
      ],
      "metadata": {
        "id": "Kb7FUnH9XqOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assumption: at least one document d in 'thresh_test' has its most-similar document in 'thresh_test'\n",
        "# for each document in 'thresh_test', compute the maximum similarity value obtained\n",
        "best_list = [max(lst) for lst in sim_dict.values()]\n",
        "best_list.sort()\n",
        "\n",
        "# set theta as the minimum of the previous list (conservative approach)\n",
        "theta = min(best_list)\n",
        "print(theta)\n",
        "\n",
        "# the choice is empirically supported\n",
        "# almost all documents in the following computations have at least one similar documents (similarity above the threshold theta)\n",
        "# still, there will be documents with many similar vectors and a few documents with no similar vectors (negligible)"
      ],
      "metadata": {
        "id": "pT93W49bAXn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Ground truth"
      ],
      "metadata": {
        "id": "EPArP46HGMvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the writing function to store similar documents\n",
        "def write(true_pairs_dict, file_path=true_sim_file):\n",
        "  f = open(file_path, \"a\")\n",
        "\n",
        "  for test_doc in true_pairs_dict.items():\n",
        "    f.write(json.dumps(test_doc))\n",
        "    f.write('\\n')\n",
        "\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "r-KDjK6x_P8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each document in the test set, compute the similar documents in the train set\n",
        "true_pairs_dict = {}\n",
        "\n",
        "# mini-batch approach:\n",
        "# a full computation takes too many time; the b_range parameter helps to focus on a subset of the documents\n",
        "b_range = (0, 20)\n",
        "\n",
        "for i in range(b_range[0], b_range[1]):\n",
        "  \n",
        "  # get the i-th document in the test set\n",
        "  u = test_tfidf[i:i+1].toarray()\n",
        "\n",
        "  # each value of true_pairs_dict is a list\n",
        "  # the first entry is a list of couple (train document similar to u and similarity value)\n",
        "  # the second entry is the time used to performed the computation\n",
        "  # this structure is flexible enough to perform all the following comparison and for any future extensions of this project\n",
        "  true_pairs_dict.setdefault(i, [[], 0.0])\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # for each document in the train set, compute the similarity\n",
        "  for j in range(train_tfidf.shape[0]):\n",
        "    v = train_tfidf[j:j+1].toarray()\n",
        "    val = round(cos_sim(u, v).item(), 2)\n",
        "    \n",
        "    # store in true_pairs_dict the index of the document and the similarity value\n",
        "    if val >= theta:\n",
        "      true_pairs_dict[i][0].append([j, val])\n",
        "  \n",
        "  # store in true_pairs_dict the time needed to complete the computation for the i-th document\n",
        "  true_pairs_dict[i][1] = round(time.time() - start, 2)\n",
        "  \n",
        "  # write the dictionary after some computations\n",
        "  # uncomment the following lines to perform the actual writing\n",
        "  '''\n",
        "  if i % 10 == 0:\n",
        "    write(true_pairs_dict)\n",
        "    true_pairs_dict = {}\n",
        "  '''"
      ],
      "metadata": {
        "id": "EixokX__HBZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Signature Matrix"
      ],
      "metadata": {
        "id": "P30_E4V_CjPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class related to the signature computation\n",
        "class SignatureClass:\n",
        "  def __init__(self, m, dim, projections=[]):\n",
        "    self.m = m\n",
        "    self.dim = dim\n",
        "\n",
        "    # create the set of vectors to compute the signature (or assign the set of vectors given as parameter)\n",
        "    if projections == []:\n",
        "      self.projections = np.random.randn(self.dim, self.m)\n",
        "    else:\n",
        "      self.projections = np.array(projections)\n",
        "\n",
        "  # compute the dot product with the given vector v and create the signature as a bit sequence \n",
        "  def generate_signature(self, v):\n",
        "    return ''.join(np.where(np.dot(v, self.projections) > 0, \"1\", \"0\"))"
      ],
      "metadata": {
        "id": "N--aMgaHCqzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) LSH"
      ],
      "metadata": {
        "id": "alxkKNTsdbbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1) Create LSH class"
      ],
      "metadata": {
        "id": "4-Lh-lWHjZgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class related to LSH\n",
        "# (note that both Signature class LSH class can be instantiated from scratch \n",
        "# or reusing previously computed variables retrieved from a file -> section 9.3)\n",
        "class LSH:\n",
        "  def __init__(self, so, b, r, theta, train_tfidf, lst_dict=[]):\n",
        "    self.so = so\n",
        "    self.r = r\n",
        "    self.b = b\n",
        "    self.theta = theta\n",
        "    self.train_tfidf = train_tfidf\n",
        "    self.lst_dict = lst_dict\n",
        "\n",
        "    # performing LSH means to take the signatures of two vectors, split them into bands and compare the related dictionaries\n",
        "    # most of the time is spent in creating the signature matrix M for each document\n",
        "    # for this reason the idea is to precompute the signatures and the dictionaries (lst_dict) for each band for every doc in training set\n",
        "    # this means speeding up the process of computing the similar documents when we call the function 'compute_sim' with the test document d\n",
        "    # creating an instance of LSH class with this precomputation takes about 6 minutes with the predefined parameters\n",
        "    # after this precomputation, the function 'compute_sim' takes a few seconds to perform\n",
        "    # we can also reuse lst_dict if we store it in a file (sections 9.2 and 9.3)\n",
        "    if self.lst_dict == []:\n",
        "      M = []\n",
        "\n",
        "      for j in range(train_tfidf.shape[0]):\n",
        "        v = train_tfidf[j:j+1].toarray()[0]\n",
        "        v_sig = so.generate_signature(v)\n",
        "        M.append(self.__partition(v_sig, r))\n",
        "    \n",
        "      for i in range(self.b):\n",
        "        self.lst_dict.append(collections.defaultdict(set))\n",
        "\n",
        "      for j, elem in enumerate(M):\n",
        "        for idx in range(self.b):\n",
        "          self.lst_dict[idx][self.__get_n_hash(elem[idx])].add(j)\n",
        "  \n",
        "  # split the signature into b bands of r bits each\n",
        "  def __partition(self, sig, r):\n",
        "    return [sig[i:i+r] for i in range(0, len(sig), r)]\n",
        "  \n",
        "  # turn bit_seq into decimal integer\n",
        "  def __get_n_hash(self, bit_seq):\n",
        "    return int(bit_seq, 2)\n",
        "\n",
        "  def cos_sim(self, u, v):\n",
        "    norm = np.linalg.norm(u) * np.linalg.norm(v)\n",
        "    cosine = np.dot(u, v.T) / norm\n",
        "    ang = np.arccos(cosine)\n",
        "    return 1 - ang/np.pi\n",
        "  \n",
        "  # compute the train documents similar to vec\n",
        "  def compute_sim(self, vec):\n",
        "    start = time.time()\n",
        "\n",
        "    # generate the signature of vec and split it into bands\n",
        "    u_sig = self.so.generate_signature(vec[0])\n",
        "    u_band = self.__partition(u_sig, self.r)\n",
        "    sim_set = set()\n",
        "\n",
        "    # using list_dict, check for each band the documents that shares with vec the same bucket\n",
        "    for idx, band in enumerate(u_band):\n",
        "      '''\n",
        "      if self.lst_dict[idx].get(self.__get_n_hash(band)) == None:\n",
        "        continue\n",
        "      '''\n",
        "\n",
        "      for elem in self.lst_dict[idx].get(self.__get_n_hash(band)):\n",
        "        sim_set.add(elem)\n",
        "\n",
        "    # discard the false positive computing the real similarity between vec and the candidate documents\n",
        "    sim_list = []\n",
        "    for doc in sim_set:\n",
        "      v = self.train_tfidf[doc].toarray()\n",
        "      val = round(self.cos_sim(vec, v).item(), 2)\n",
        "    \n",
        "      if val >= self.theta:\n",
        "        sim_list.append(doc)\n",
        "\n",
        "    end = round(time.time() - start, 2)\n",
        "    return [sim_list, end]"
      ],
      "metadata": {
        "id": "-T3jx9A1dind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2) Create new instances"
      ],
      "metadata": {
        "id": "LgNcOwweje8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the instances of the signature class and the LSH class\n",
        "so = SignatureClass(m, test_tfidf.shape[1])\n",
        "lsh = LSH(so, b, r, theta, train_tfidf)"
      ],
      "metadata": {
        "id": "bp_z7oqCtklx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the variables of so (projections) and lsh (list_dict)\n",
        "# uncomment the following lines to perform the actual writing\n",
        "'''\n",
        "f = open(sig_file, \"w\")\n",
        "\n",
        "# write so.projections in the first line of the file\n",
        "f.write(json.dumps(so.projections.tolist()))\n",
        "f.write('\\n')\n",
        "\n",
        "# write the dictionaries of list_dict in the following lines (one per line)\n",
        "for d in lsh.lst_dict:\n",
        "  tmp = {}\n",
        "\n",
        "  for k, v in d.items():\n",
        "    tmp.setdefault(k, list(v))\n",
        "\n",
        "  f.write(json.dumps(tmp))\n",
        "  f.write('\\n')\n",
        "\n",
        "f.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "zTWuhUPZ_vqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3) Retrieve old instances"
      ],
      "metadata": {
        "id": "ik00Qqzaj8Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the variables of so (projections) and lsh (list_dict)\n",
        "f = open(sig_file, \"r\")\n",
        "\n",
        "list_dict = []\n",
        "projections = []\n",
        "\n",
        "i = 0\n",
        "for line in f:\n",
        "  if i == 0: # read so.projections from the first line of the file\n",
        "    projections = json.loads(line)\n",
        "    i += 1\n",
        "  else: # read the dictionaries of list_dict from the following lines (one per line)\n",
        "    d = json.loads(line)\n",
        "    tmp = {}\n",
        "\n",
        "    for k, v in d.items():\n",
        "      tmp.setdefault(int(k), list(v))\n",
        "    list_dict.append(tmp)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rOvB_4vpS5mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the instances of the signature class and the LSH class using the parameters retrieved from the file\n",
        "so = SignatureClass(m, test_tfidf.shape[1], projections)\n",
        "lsh = LSH(so, b, r, theta, train_tfidf, list_dict)"
      ],
      "metadata": {
        "id": "wKMhzTDHxP9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Comparison"
      ],
      "metadata": {
        "id": "iTbk9z3Ydedd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1) LSH result"
      ],
      "metadata": {
        "id": "wa9f_ioLe9pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the jaccard coefficient\n",
        "def jaccard(sim_list1, sim_list2):\n",
        "  set1, set2 = (set(sim_list1), set(sim_list2))\n",
        "\n",
        "  # as noted in section 6, some test documents may have no similar train documents\n",
        "  # in this case, the correct result for LSH is to return an empty set as the ground truth computation\n",
        "  # thus, if both sets are empty, the jaccard coefficient is defined to be 1\n",
        "  if len(set1) == 0 and len(set2) == 0:\n",
        "    return 1.0\n",
        "  \n",
        "  return len(set1.intersection(set2)) / len(set1.union(set2))"
      ],
      "metadata": {
        "id": "jpvfvCYgrgdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the writing function for LSH computations and comparison between LSH and ground truth\n",
        "def write(doc, lsh_res, file_path):\n",
        "  f = open(file_path, \"a\")\n",
        "\n",
        "  f.write(json.dumps([doc, lsh_res]))\n",
        "  f.write('\\n')\n",
        "\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "SYI_je0DsntG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function to compare LSH and ground truth for document doc\n",
        "def compare(doc):\n",
        "\n",
        "  # get the real similar documents\n",
        "  line = linecache.getline(true_sim_file, doc+1)\n",
        "  test_doc = json.loads(line)\n",
        "\n",
        "  ground_sim = [elem[0] for elem in test_doc[1][0]]\n",
        "  ground_t = test_doc[1][1]\n",
        "\n",
        "  # get the LSH results\n",
        "  lsh_sim, lsh_t = lsh.compute_sim(test_tfidf[doc].toarray())\n",
        "\n",
        "  # uncomment the following line to perform the actual writing\n",
        "  # write(doc, [lsh_sim, lsh_t], lsh_file)\n",
        "\n",
        "  # compute the jaccard coefficient between the two sets and compare how inefficient is the standard approach wrt LSH\n",
        "  jac = jaccard(ground_sim, lsh_sim)\n",
        "  time = ground_t / lsh_t\n",
        "\n",
        "  # uncomment the following line to perform the actual writing\n",
        "  # write(doc, [jac, time], comparison_file)\n",
        "  \n",
        "  print(jac)\n",
        "  print(time)"
      ],
      "metadata": {
        "id": "gaXx77uydi9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the index of the document to test\n",
        "doc = 0\n",
        "compare(doc)"
      ],
      "metadata": {
        "id": "-5DqBE4QD-fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0aa59b6-e4e8-4a0b-8d20-6bd12e01e601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8455284552845529\n",
            "1.781063122923588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2) Aggregated results"
      ],
      "metadata": {
        "id": "Hp-r6ByMfmxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read(file_path, idx):\n",
        "  f = open(file_path, \"r\")\n",
        "  s = 0\n",
        "\n",
        "  for line in f:\n",
        "    test_doc = json.loads(line)\n",
        "    s += test_doc[1][idx]\n",
        "    \n",
        "  f.close()\n",
        "  return s\n",
        "\n",
        "print('Average time for exact similarity computation: {:.2f}'.format(read(true_sim_file, 1) / test_tfidf.shape[0]))\n",
        "print('Average time for LSH similarity computation: {:.2f}'.format(read(lsh_file, 1) / test_tfidf.shape[0]))\n",
        "print('Average Jaccard coefficient: {:.2f}'.format(read(comparison_file, 0) / test_tfidf.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ycVVhgpYqp",
        "outputId": "54ac46e6-b9b7-425c-e37f-569c9d9b8726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average time for exact similarity computation: 73.19\n",
            "Average time for LSH similarity computation: 27.70\n",
            "Average Jaccard coefficient: 0.73\n"
          ]
        }
      ]
    }
  ]
}